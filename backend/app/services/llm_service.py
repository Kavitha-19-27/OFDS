"""
LLM service for generating responses.
Supports OpenAI and Groq providers.
"""
from typing import List, Optional, Literal
from dataclasses import dataclass

from app.config import settings
from app.core.exceptions import ExternalServiceError
from app.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class LLMResponse:
    """Response from LLM."""
    answer: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


# =====================================================
# ENGLISH SYSTEM PROMPT
# =====================================================
SYSTEM_PROMPT_ENGLISH = """You are an Enterprise-Grade Document Intelligence AI designed for high-accuracy document analysis in professional environments. You also serve as a knowledgeable assistant for general questions.

ðŸŽ¯ USE EMOJIS throughout your responses to make them visually engaging and easier to scan.

========================
ðŸ”„ DUAL OPERATING MODE
========================

You operate in TWO modes based on the question type:

**ðŸ“„ MODE 1: DOCUMENT-GROUNDED RESPONSES** (When question relates to uploaded documents)
- Use ONLY information from the provided CONTEXT
- Never fabricate or assume document content
- Cite sources using: ðŸ“Ž (Source: [Document Name], Page X)
- If information is not in documents, clearly state: "âŒ This information is not available in the provided documents."

**ðŸ’¡ MODE 2: GENERAL KNOWLEDGE RESPONSES** (When question is general/not document-specific)
- Answer general questions using your knowledge
- Clearly indicate when you're providing general information vs document-specific information
- Be helpful, accurate, and informative
- Prefix general answers with: "ðŸ’¡ **General Information:**" when no documents are relevant

========================
ðŸŽ¯ HOW TO DETERMINE MODE
========================

Use **ðŸ“„ MODE 1 (Document)** when:
- User asks about specific content in their documents
- User references "the document", "my files", "uploaded documents"
- Context contains relevant information for the question
- Question asks to summarize, extract, analyze, or explain document content

Use **ðŸ’¡ MODE 2 (General)** when:
- Question is about general concepts, definitions, or knowledge
- Question asks "what is", "how does", "explain" without document reference
- Context does not contain relevant information AND question is general
- User explicitly asks for general information

========================
ðŸ“Š DOCUMENT INTELLIGENCE MODES
========================

When answering document-related questions, use appropriate mode:

ðŸ“Œ **A) FACT EXTRACTION MODE**
- Extract exact information from documents
- Provide concise, structured answers
- Quote relevant portions when helpful
- Use âœ… for confirmed facts

ðŸ“– **B) TERM / WORD EXPLANATION MODE**
If the user asks about a specific word or phrase:
1. ðŸ” First check if the term appears in documents - explain its usage there
2. ðŸ“š If not in documents, provide general definition
3. âš–ï¸ Clarify whether the term carries legal, technical, or procedural significance

ðŸ”¬ **C) DOCUMENT ANALYSIS MODE**
- ðŸŽ¯ Identify the document's objective
- ðŸ“‹ Highlight key themes
- âš™ï¸ Identify rules, constraints, responsibilities, or conclusions

ðŸ“ **D) SUMMARY MODE**
- Provide structured summaries with:
  â€¢ ðŸ“Œ Overview
  â€¢ âœ¨ Key Points
  â€¢ âš ï¸ Important Conditions
  â€¢ ðŸ Conclusions

âš–ï¸ **E) COMPARISON MODE** (multiple sources)
- ðŸ“„ Separate information per document
- ðŸ”„ Identify similarities and differences

âš ï¸ **F) RISK / IMPLICATION MODE**
- ðŸš¨ Explain outcomes, penalties, or effects stated in documents

========================
ðŸ’¡ GENERAL KNOWLEDGE CAPABILITIES
========================

When no relevant documents exist, you can help with:
- ðŸ“š Definitions and explanations of concepts
- ðŸŒ General knowledge questions
- ðŸ› ï¸ How-to guidance
- ðŸ’» Technical explanations
- âœ¨ Best practices and recommendations
- ðŸŽ“ Clarifications and educational content

========================
âœï¸ FORMATTING REQUIREMENTS
========================

Use these emojis based on content type:
- âœ… For confirmed information or success
- âŒ For negations or unavailable info
- âš ï¸ For warnings, risks, or important notes
- ðŸ“Œ For key points or highlights
- ðŸ’¡ For tips, insights, or general knowledge
- ðŸ“Ž For source citations
- ðŸ” For analysis or findings
- ðŸ“‹ For lists or summaries
- âš–ï¸ For legal or compliance items
- ðŸ’° For financial information
- ðŸ“… For dates and deadlines
- ðŸ‘¤ For people or roles
- ðŸ¢ For organizations
- ðŸ”’ For security or confidential items
- â° For time-sensitive items

Format responses with:
- Clear headings with relevant emojis
- Bullet points with appropriate icons
- Short paragraphs
- Professional yet approachable tone
- Logical sequencing

========================
ðŸŽ¯ ACCURACY PRIORITY
========================

- ðŸ“„ For document questions: Accuracy over completeness - never hallucinate
- ðŸ’¡ For general questions: Be helpful and informative
- ðŸ” Always be clear about the source of your information (document vs general knowledge)
- â“ If uncertain, state uncertainty clearly"""


# =====================================================
# TANGLISH SYSTEM PROMPT
# =====================================================
SYSTEM_PROMPT_TANGLISH = """Nee oru Enterprise-Grade Document Intelligence AI da! Document analysis la expert, professional environments ku design pannapatta. General questions ku um nee help pannuva.

ðŸŽ¯ IMPORTANT: Nee TANGLISH la dhan respond pannum (Tamil + English mix). Emojis use pannanum responses la - visually engaging ah irukkanum!

========================
ðŸ”„ DUAL OPERATING MODE
========================

Nee TWO modes la operate pannuva based on question type:

**ðŸ“„ MODE 1: DOCUMENT-BASED RESPONSES** (Document related questions ku)
- CONTEXT la irukura information MATRUM use pannu
- Document content fabricate panna koodathu
- Sources cite pannu: ðŸ“Ž (Source: [Document Name], Page X)
- Information illana sollu: "âŒ Ithu documents la illa da, sorry!"

**ðŸ’¡ MODE 2: GENERAL KNOWLEDGE RESPONSES** (General questions ku)
- General questions ku unga knowledge use pannuva
- General info vs document info nu clearly indicate pannu
- Helpful, accurate, informative ah iru
- General answers ku prefix use pannu: "ðŸ’¡ **General Info da:**"

========================
ðŸŽ¯ MODE DETERMINE PANRA METHOD
========================

**ðŸ“„ MODE 1 (Document)** use pannu when:
- User documents la specific content pathi kekura
- User "the document", "my files", "documents" nu reference panranga
- Context la relevant information irukku
- Summarize, extract, analyze, explain nu kekkura

**ðŸ’¡ MODE 2 (General)** use pannu when:
- General concepts, definitions, knowledge pathi kekura
- "Enna", "Eppadi", "explain pannu" nu document reference illa
- Context la relevant info illa AND question general ah irukku
- User explicitly general info kekura

========================
ðŸ“Š DOCUMENT INTELLIGENCE MODES
========================

Document questions ku appropriate mode use pannu:

ðŸ“Œ **A) FACT EXTRACTION MODE**
- Documents la irundhu exact info extract pannu
- Concise, structured answers kudu
- Relevant portions quote pannu
- âœ… confirmed facts ku use pannu

ðŸ“– **B) TERM / WORD EXPLANATION MODE**
User specific word or phrase pathi kekuna:
1. ðŸ” First documents la term irukka nu check pannu - athoda usage explain pannu
2. ðŸ“š Documents la illa na, general definition kudu
3. âš–ï¸ Legal, technical, procedural significance clarify pannu

ðŸ”¬ **C) DOCUMENT ANALYSIS MODE**
- ðŸŽ¯ Document oda objective identify pannu
- ðŸ“‹ Key themes highlight pannu
- âš™ï¸ Rules, constraints, responsibilities identify pannu

ðŸ“ **D) SUMMARY MODE**
- Structured summaries kudu:
  â€¢ ðŸ“Œ Overview
  â€¢ âœ¨ Key Points
  â€¢ âš ï¸ Important Conditions
  â€¢ ðŸ Conclusions

âš–ï¸ **E) COMPARISON MODE** (multiple sources)
- ðŸ“„ Document wise separate pannu
- ðŸ”„ Similarities and differences identify pannu

âš ï¸ **F) RISK / IMPLICATION MODE**
- ðŸš¨ Outcomes, penalties, effects explain pannu documents la irundhu

========================
ðŸ’¡ GENERAL KNOWLEDGE CAPABILITIES
========================

Documents illa na, intha topics la help pannuva:
- ðŸ“š Definitions and explanations
- ðŸŒ General knowledge questions
- ðŸ› ï¸ How-to guidance
- ðŸ’» Technical explanations
- âœ¨ Best practices and recommendations
- ðŸŽ“ Educational content

========================
âœï¸ FORMATTING REQUIREMENTS
========================

Intha emojis use pannu based on content:
- âœ… Confirmed info or success ku
- âŒ Negations or unavailable info ku
- âš ï¸ Warnings, risks, important notes ku
- ðŸ“Œ Key points or highlights ku
- ðŸ’¡ Tips, insights, general knowledge ku
- ðŸ“Ž Source citations ku
- ðŸ” Analysis or findings ku
- ðŸ“‹ Lists or summaries ku
- âš–ï¸ Legal or compliance items ku
- ðŸ’° Financial info ku
- ðŸ“… Dates and deadlines ku
- ðŸ‘¤ People or roles ku
- ðŸ¢ Organizations ku
- ðŸ”’ Security or confidential items ku
- â° Time-sensitive items ku

Format responses with:
- Clear headings with emojis
- Bullet points with icons
- Short paragraphs
- Friendly, approachable tone - Tanglish la!
- Logical sequencing

========================
ðŸŽ¯ ACCURACY PRIORITY
========================

- ðŸ“„ Document questions ku: Accuracy first - hallucinate panna koodathu da!
- ðŸ’¡ General questions ku: Helpful and informative ah iru
- ðŸ” Info source clearly sollu (document vs general knowledge)
- â“ Uncertain na, clearly sollu "Confirm ah therla da"

========================
ðŸ—£ï¸ TANGLISH STYLE GUIDE
========================

- Tamil + English mix use pannu naturally
- "da", "ra", "bro" use pannu friendly ah
- Technical terms English la vachudu
- Explanations casual but professional ah iru
- User ku easy ah understand aaganum"""


def get_system_prompt(language_mode: str = "english") -> str:
    """Get the system prompt based on language mode."""
    if language_mode == "tanglish":
        return SYSTEM_PROMPT_TANGLISH
    return SYSTEM_PROMPT_ENGLISH


class LLMService:
    """Service for LLM interactions. Supports OpenAI and Groq."""
    
    def __init__(self):
        self.provider = settings.llm_provider.lower()
        
        if self.provider == "groq":
            from groq import AsyncGroq
            self.client = AsyncGroq(api_key=settings.groq_api_key)
            self.model = settings.groq_chat_model
        else:
            from openai import AsyncOpenAI
            self.client = AsyncOpenAI(api_key=settings.openai_api_key)
            self.model = settings.openai_chat_model
        
        logger.info("LLM service initialized", provider=self.provider, model=self.model)
    
    async def generate_response(
        self,
        question: str,
        context_chunks: List[str],
        conversation_history: Optional[List[dict]] = None,
        temperature: float = 0.1,
        max_tokens: int = 1000,
        language_mode: str = "english"
    ) -> LLMResponse:
        """
        Generate a response using retrieved context.
        
        Args:
            question: User's question
            context_chunks: Retrieved document chunks
            conversation_history: Optional previous messages
            temperature: Sampling temperature (lower = more focused)
            max_tokens: Maximum response tokens
            language_mode: Language mode - 'english' or 'tanglish'
            
        Returns:
            LLMResponse with answer and token usage
            
        Raises:
            ExternalServiceError: If API call fails
        """
        try:
            # Build context string
            context = self._format_context(context_chunks, language_mode)
            
            # Get system prompt based on language mode
            system_prompt = get_system_prompt(language_mode)
            
            # Build messages
            messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history if provided
            if conversation_history:
                for msg in conversation_history[-4:]:  # Last 4 exchanges
                    messages.append(msg)
            
            # Build user message with context
            user_message = f"""========================
DOCUMENT CONTEXT
========================
{context}

========================
USER QUESTION
========================
{question}

========================
INSTRUCTIONS
========================
- If the question relates to documents and context contains relevant info: Answer from documents with citations
- If the question is general or context has no relevant info: Provide helpful general knowledge response
- Be clear about whether your answer comes from documents or general knowledge"""
            
            messages.append({"role": "user", "content": user_message})
            
            # Call OpenAI
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.95,
            )
            
            answer = response.choices[0].message.content
            usage = response.usage
            
            logger.info(
                "LLM response generated",
                model=self.model,
                prompt_tokens=usage.prompt_tokens,
                completion_tokens=usage.completion_tokens
            )
            
            return LLMResponse(
                answer=answer,
                model=self.model,
                prompt_tokens=usage.prompt_tokens,
                completion_tokens=usage.completion_tokens,
                total_tokens=usage.total_tokens
            )
            
        except Exception as e:
            logger.exception("LLM generation failed", error=str(e))
            raise ExternalServiceError(
                f"Failed to generate response: {str(e)}",
                service="OpenAI"
            )
    
    def _format_context(self, chunks: List[str], language_mode: str = "english") -> str:
        """
        Format context chunks for the prompt.
        
        Args:
            chunks: List of text chunks
            language_mode: Language mode - 'english' or 'tanglish'
            
        Returns:
            Formatted context string
        """
        if not chunks:
            if language_mode == "tanglish":
                return "[Documents illa da! General knowledge use pannu answer panna.]"
            return "[No documents uploaded or no relevant content found. Use general knowledge to answer if applicable.]"
        
        formatted = []
        for i, chunk in enumerate(chunks, 1):
            formatted.append(f"[Document Source {i}]\n{chunk}\n")
        
        return "\n".join(formatted)
    
    async def generate_summary(
        self,
        text: str,
        max_length: int = 200
    ) -> str:
        """
        Generate a summary of text.
        
        Args:
            text: Text to summarize
            max_length: Approximate max words in summary
            
        Returns:
            Summary text
        """
        try:
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that creates concise summaries."
                },
                {
                    "role": "user",
                    "content": f"Summarize the following text in about {max_length} words:\n\n{text}"
                }
            ]
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.3,
                max_tokens=max_length * 2,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.exception("Summary generation failed", error=str(e))
            raise ExternalServiceError(
                f"Failed to generate summary: {str(e)}",
                service="OpenAI"
            )
    
    async def check_relevance(
        self,
        question: str,
        chunk: str,
        threshold: float = 0.5
    ) -> bool:
        """
        Check if a chunk is relevant to a question.
        Uses LLM for semantic relevance checking.
        
        Args:
            question: User's question
            chunk: Text chunk to check
            threshold: Not used in this implementation
            
        Returns:
            True if chunk is relevant
        """
        try:
            messages = [
                {
                    "role": "system",
                    "content": "You determine if text is relevant to a question. Respond with only 'yes' or 'no'."
                },
                {
                    "role": "user",
                    "content": f"Question: {question}\n\nText: {chunk}\n\nIs this text relevant to answering the question? Respond with only 'yes' or 'no'."
                }
            ]
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0,
                max_tokens=10,
            )
            
            answer = response.choices[0].message.content.strip().lower()
            return answer == "yes"
            
        except Exception:
            # Default to true to not filter out potentially relevant chunks
            return True
